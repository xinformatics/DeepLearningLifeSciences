# -*- coding: utf-8 -*-
"""chapter7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f5FA1sTXjKEzzATJRNyz2g3e0uHpYEL4
"""

# DL for Microscopy
#cell counting in deepchem

# Commented out IPython magic to ensure Python compatibility.
##setup tensorflow v1
# %tensorflow_version 1.x

!wget -c https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh
!chmod +x Anaconda3-2019.10-Linux-x86_64.sh
!bash ./Anaconda3-2019.10-Linux-x86_64.sh -b -f -p /usr/local
!conda install -y -c deepchem -c rdkit -c conda-forge -c omnia deepchem-gpu=2.3.0
import sys
sys.path.append('/usr/local/lib/python3.7/site-packages/')

import deepchem as dc
dc.__version__ #should match with the installed

#download image data
!wget https://data.broadinstitute.org/bbbc/BBBC005/BBBC005_v1_images.zip

!unzip BBBC005_v1_images.zip

import numpy as np
import os
import re

image_dir = 'BBBC005_v1_images'
files = []
labels = []
for f in os.listdir(image_dir):
  if f.endswith('.TIF'):
    files.append(os.path.join(image_dir, f))
    labels.append(int(re.findall('_C(.*?)_', f)[0]))
loader = dc.data.ImageLoader()
dataset = loader.featurize(files, np.array(labels))

import deepchem.models.tensorgraph.layers as layers

splitter = dc.splits.RandomSplitter()
train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(dataset, seed=123)

del model

learning_rate = dc.models.optimizers.ExponentialDecay(0.001, 0.9,250)
#above line is different in book
#model = dc.models.TensorGraph(learning_rate=learning_rate, model_dir='model')
# for loading the pre trained model as mentioned in the book
model = dc.models.TensorGraph(learning_rate=learning_rate, model_dir='models/model')
features = layers.Feature(shape=(None, 520, 696))
labels = layers.Label(shape=(None,))

prev_layer = features
for num_outputs in [16, 32, 64, 128, 256]:
  prev_layer = layers.Conv2D(num_outputs, kernel_size=5, stride=2,in_layers=prev_layer)

output = layers.Dense(1, in_layers=layers.Flatten(prev_layer))

model.add_output(output)

loss = layers.ReduceSum(layers.L2Loss(in_layers=(output, labels)))

model.set_loss(loss)

#model training
#model.fit(train_dataset, nb_epoch=50)

#testing
y_pred = model.predict(test_dataset).flatten()
print(np.sqrt(np.mean((y_pred-test_dataset.y)**2)))

pwd

#!wget https://s3-us-west-1.amazonaws.com/deepchem.io/featurized_datasets/microscopy_models.zip
#!unzip microscopy_models.zip

pwd

del model

#load model, above procedure gives error so follow another idea
learning_rate = dc.models.optimizers.ExponentialDecay(0.001, 0.9, 250)
model = dc.models.TensorGraph(learning_rate=learning_rate, model_dir='model')
features = layers.Feature(shape=(None, 520, 696))
labels = layers.Label(shape=(None,))
prev_layer = features
for num_outputs in [16, 32, 64, 128, 256]:
  prev_layer = layers.Conv2D(num_outputs, kernel_size=5, stride=2, in_layers=prev_layer)
output = layers.Dense(1, in_layers=layers.Flatten(prev_layer))
model.add_output(output)
loss = layers.ReduceSum(layers.L2Loss(in_layers=(output, labels)))
model.set_loss(loss)

cd ../

#!unzip microscopy_models.zip

model.get_checkpoints()

#model.load_from_pretrained()

## then restore a model
model.restore()

#using pretrained model
# mkdir models
# cd models
# wget https://s3-us-west-1.amazonaws.com/deepchem.io/featurized_datasets/microscopy_models.zip
# unzip microscopy_models.zip
## then restore a model
#model.restore()

#predict on the model------ pre trained model did not load
# I trained from scratch it took 3 hours for 50 epochs
#y_pred = model.predict(test_dataset).flatten()
#print(np.sqrt(np.mean((y_pred-test_dataset.y)**2)))

y_pred[0:10]

#another thing would be to load pretrained model, 
# train it little more and then predict
# model.fit(train_dataset, nb_epoch=50)
# y_pred = model.predict(test_dataset).flatten()
# print(np.sqrt(np.mean((y_pred-test_dataset.y)**2)))

# metric in the 3 cases :
# 1. Training from scratch (totalepochs = 50) and predict: 2.7211521453040683
# 2. Load pretrained (total epochs = x maybe 50) and predict: NA
# 3. Load and then retrain (total epochs = x+50 maybe (50+50)) and predict: NA

# Couldn't debug the error of loading the model provided by authors on github
# I can load the model I trained but not succesful with the model provided on github
# going forward with segmentation task

#!wget https://data.broadinstitute.org/bbbc/BBBC005/BBBC005_v1_ground_truth.zip

#!unzip BBBC005_v1_ground_truth.zip

image_dir = 'BBBC005_v1_images'
label_dir = 'BBBC005_v1_ground_truth'
rows = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L','M', 'N', 'O', 'P')
blurs = (1, 4, 7, 10, 14, 17, 20, 23, 26, 29, 32, 35, 39, 42, 45, 48)
files = []
labels = []
for f in os.listdir(label_dir):
  if f.endswith('.TIF'):
    for row, blur in zip(rows, blurs):
      fname = f.replace('_F1', '_F%d'%blur).replace('_A', '_%s'%row)
      files.append(os.path.join(image_dir, fname))
      labels.append(os.path.join(label_dir, f))
      
loader = dc.data.ImageLoader()
dataset = loader.featurize(files, labels)

splitter = dc.splits.RandomSplitter()
train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(dataset, seed=123)

#Unet architecture

del model

learning_rate = dc.models.optimizers.ExponentialDecay(0.01, 0.9, 250)
model = dc.models.TensorGraph(learning_rate=learning_rate, model_dir='segmentation_old')
features = layers.Feature(shape=(None, 520, 696, 1)) / 255.0
labels = layers.Label(shape=(None, 520, 696, 1)) / 255.0
# Downsample three times.
conv1 = layers.Conv2D(16, kernel_size=5, stride=2, in_layers=features)
conv2 = layers.Conv2D(32, kernel_size=5, stride=2, in_layers=conv1)
conv3 = layers.Conv2D(64, kernel_size=5, stride=2, in_layers=conv2)
# Do a 1x1 convolution.
conv4 = layers.Conv2D(64, kernel_size=1, stride=1, in_layers=conv3)
# Upsample three times.
concat1 = layers.Concat(in_layers=[conv3, conv4], axis=3)
deconv1 = layers.Conv2DTranspose(32, kernel_size=5, stride=2, in_layers=concat1)
concat2 = layers.Concat(in_layers=[conv2, deconv1], axis=3)
deconv2 = layers.Conv2DTranspose(16, kernel_size=5, stride=2, in_layers=concat2)
concat3 = layers.Concat(in_layers=[conv1, deconv2], axis=3)
deconv3 = layers.Conv2DTranspose(1, kernel_size=5, stride=2, in_layers=concat3)
# Compute the final output.
concat4 = layers.Concat(in_layers=[features, deconv3], axis=3)
logits = layers.Conv2D(1, kernel_size=5, stride=1, activation_fn=None, in_layers=concat4)
output = layers.Sigmoid(logits)
model.add_output(output)
loss = layers.ReduceSum(layers.SigmoidCrossEntropy(in_layers=(labels, logits)))
model.set_loss(loss)

pwd

#model.restore()

# same error coming in loading pretrained model
# skipping this chapter lots of issues

# training Unet from scratch for 5 epochs just to see how it works

model.fit(train_dataset, nb_epoch=5, checkpoint_interval=100)

scores = []
for x, y, w, id in test_dataset.itersamples():
  y_pred = model.predict_on_batch([x]).squeeze()
  scores.append(np.mean((y>0) == (y_pred>0.5)))
print(np.mean(scores))

# after 5 epochs from scratch 0.9882 ~ 98.82%
# as per book after 50 epochs ~ 0.9899 98.99%

#chapter over but unable to load pretrained models, will come back to this

