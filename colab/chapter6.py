# -*- coding: utf-8 -*-
"""chapter6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cVMFna8K6X61j6rIejJLd5UT9GXz4_3q
"""

# to start with deep learning for genomics
# deep learning to learn a model directly from data.

# Commented out IPython magic to ensure Python compatibility.
##setup tensorflow v1
# %tensorflow_version 1.x

!wget -c https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh
!chmod +x Anaconda3-2019.10-Linux-x86_64.sh
!bash ./Anaconda3-2019.10-Linux-x86_64.sh -b -f -p /usr/local
!conda install -y -c deepchem -c rdkit -c conda-forge -c omnia deepchem-gpu=2.3.0
import sys
sys.path.append('/usr/local/lib/python3.7/site-packages/')

import deepchem as dc
dc.__version__ #should match with the installed

import numpy as np

# We will try to train a model that predicts those labels based on the sequence of each
# segment.

#upload data to drive as zip file and then unzip

#extractall() with any argument will extract everything in the same directory 
# this is what is expected
import zipfile
with zipfile.ZipFile('tocolabch6.zip', 'r') as zip_ref:
    zip_ref.extractall()

import tensorflow as tf
import deepchem.models.tensorgraph.layers as layers

#del model

# dna-transcription factor binding with chromation accesibility
model = dc.models.TensorGraph(batch_size=1000)
features = layers.Feature(shape=(None, 101, 4))
#accessibility = layers.Feature(shape=(None, 1)) #data is added later
labels = layers.Label(shape=(None, 1))
weights = layers.Weights(shape=(None, 1))

prev = features
for i in range(3):
  prev = layers.Conv1D(filters=15, kernel_size=10,activation=tf.nn.relu, padding='same',in_layers=prev)
  prev = layers.Dropout(dropout_prob=0.5, in_layers=prev)

logits = layers.Dense(out_channels=1, in_layers=layers.Flatten(prev))
output = layers.Sigmoid(logits)
model.add_output(output)



loss = layers.SigmoidCrossEntropy(in_layers=[labels, logits])
weighted_loss = layers.WeightedError(in_layers=[loss, weights])
model.set_loss(weighted_loss)

train = dc.data.DiskDataset('train_dataset')
valid = dc.data.DiskDataset('valid_dataset')
metric = dc.metrics.Metric(dc.metrics.roc_auc_score)

# data loaded after an hour of debugging , noob! Training below till 200 epochs

for i in range(20):
  model.fit(train, nb_epoch=10)
  print (i)
  print('training: ', model.evaluate(train, [metric]))
  print('validation: ',model.evaluate(valid, [metric]))

#model is overfitting; 
# there are fundamental limits to how well this model can ever work.
# accessibility, methylation, shape, the presence of other molecules, etc.
# Any model that ignores those factors will be limited in how accurate its predictions

# incorporating chromatin accesibility
# HEPG2 cells in experiment human liver cancer cell line. 
# Hep G2 is an immortal cell line which was derived from the
# liver tissue of a 15-year-old African American adolescent 
# boy with a well-differentiated hepatocellular carcinoma.

span_accessibility = {}
for line in open('accessibility.txt'):
  fields = line.split()
  span_accessibility[fields[0]] = float(fields[1])

accessibility = layers.Feature(shape=(None, 1)) #added earlier

logits = layers.Dense(out_channels=1, in_layers=layers.Flatten(prev))
prev = layers.Concat([layers.Flatten(prev), accessibility])
logits = layers.Dense(out_channels=1, in_layers=prev)

def generate_batches(dataset, epochs):
  for epoch in range(epochs):
    for X, y, w, ids in dataset.iterbatches(batch_size=1000,pad_batches=True):
      yield {features: X,accessibility: np.array([span_accessibility[id] for id in ids]),labels: y,weights: w}

for i in range(5):
  print (i)
  model.fit_generator(generate_batches(train, 10))
  print('training:       ',model.evaluate_generator(generate_batches(train, 1), [metric],labels=[labels], weights=[weights]))
  print('validation:       ',model.evaluate_generator(generate_batches(valid, 1), [metric],labels=[labels], weights=[weights]))
  print ('\n')

#ok its training, for more epochs

for i in range(5,20):
  print (i)
  model.fit_generator(generate_batches(train, 10))
  print('training:       ',model.evaluate_generator(generate_batches(train, 1), [metric],labels=[labels], weights=[weights]))
  print('validation:       ',model.evaluate_generator(generate_batches(valid, 1), [metric],labels=[labels], weights=[weights]))
  print ('\n')

#clearly overfitting, need more data/features, may be dna structural information
# validation max at 0.74 and then decreases.

#del model

#RNA Interference: another case of using ML in genomics

#It also is a powerful tool for biology and medicine. It lets you temporarily “turn off ”
#any gene you want.

# Model to predict the power so different siRNA at 
# how effective it is at silencing its target gene.
# Small values indicate
# ineffective molecules, while larger values indicate more effective ones. The model
# takes the sequence as input and tries to predict the effectiveness.
# siRNA Regression

model = dc.models.TensorGraph()
features = layers.Feature(shape=(None, 21, 4))
labels = layers.Label(shape=(None, 1))
prev = features
for i in range(2):
  prev = layers.Conv1D(filters=10, kernel_size=10,activation=tf.nn.relu, padding='same',in_layers=prev)
  prev = layers.Dropout(dropout_prob=0.3, in_layers=prev)

output = layers.Dense(out_channels=1, activation_fn=tf.sigmoid,in_layers=layers.Flatten(prev))

model.add_output(output)

loss = layers.ReduceMean(layers.L2Loss(in_layers=[labels, output]))

model.set_loss(loss)

train = dc.data.DiskDataset('train_siRNA')
valid = dc.data.DiskDataset('valid_siRNA')
metric = dc.metrics.Metric(dc.metrics.pearsonr, mode='regression')

for i in range(20):
  print(i)
  model.fit(train, nb_epoch=10)
  print('training:   ',model.evaluate(train, [metric]))
  print('validation: ',model.evaluate(valid, [metric]))
  print('\n')

# validation correlation maxed out at 0.634, train corre increases ; classic case of overfitting

# with the amount of data available, this result is okay in my opinion

# completed chapter 6

